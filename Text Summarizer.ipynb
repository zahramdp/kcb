{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963bf7a2-a5dc-4e4d-a698-019f40ebf44e",
   "metadata": {},
   "source": [
    "Make summarizer using Natural Language Toolkit (NLTK) in python. The script takes user input for a block of text and the desired number of sentences in the summary, then generates and prints a summary by extracting the most important sentences from the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db4cdf-756f-4a0f-a539-c22810c546e9",
   "metadata": {},
   "source": [
    "First download the ntlk library using \"pip install ntlk\" and download the necessary NTLK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a1685-bae3-4bdc-9036-e777ca3adfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f4f08-ac9f-4bc8-b3c3-f22ad681ae6a",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0a608-6430-4916-afa6-361cafa6a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b8da10-ecdc-440e-b8f5-64ea9768c001",
   "metadata": {},
   "source": [
    "1. Converts the text into lowercase and tokenizes it into words.\n",
    "2. Removes stopwords and punctuation.\n",
    "3. Creates a frequency table (dictionary) where keys are words and values are the frequency of those words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50173cb4-6089-4ed6-b691-113f66a24d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_frequencies(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    freq_table = defaultdict(int)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and word not in string.punctuation:\n",
    "            freq_table[word] += 1\n",
    "    \n",
    "    return freq_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b10914-2831-4962-b2e0-0f0abb31c599",
   "metadata": {},
   "source": [
    "Tokenizes each sentence into words. Scores each sentence by summing the frequencies of its words (as provided by the frequency table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3450d-accf-4618-ac42-e72286da49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentences(sentences, freq_table):\n",
    "    sentence_scores = defaultdict(int)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        for word in words:\n",
    "            if word in freq_table:\n",
    "                sentence_scores[sentence] += freq_table[word]\n",
    "    \n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34eb619-0dfd-4c34-8ec0-7120d1ad7355",
   "metadata": {},
   "source": [
    "1. Splits the text into sentences.\n",
    "2. Computes word frequencies for the text.\n",
    "3. Scores each sentence based on word frequencies.\n",
    "4. Sorts the sentences by their scores in descending order.\n",
    "5. Selects the top n sentences and joins them into a single string to form the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ab38a-8d48-4699-9f95-0dbda47c83a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text, n):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_frequencies = compute_word_frequencies(text)\n",
    "    sentence_scores = score_sentences(sentences, word_frequencies)\n",
    "    \n",
    "    # Get the top n sentences with the highest scores\n",
    "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:n]\n",
    "    \n",
    "    return ' '.join(summary_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3547147-8b6b-4419-b00a-e4fff6c82a24",
   "metadata": {},
   "source": [
    "Prompts the user to input the text they want to summarize and prompts the user to specify the number of sentences they want in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa1bd9-a117-4524-bd0a-a1ea9aa0e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input(\"Enter the text you want to summarize: \")\n",
    "num_sentences = int(input(\"Enter the number of sentences you want in the summary: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af107b9-6b32-472a-92a7-52c515b3def1",
   "metadata": {},
   "source": [
    "Generate and print the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b2b3e1-66ce-43ea-a0a6-22de87fdca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary\n",
    "summary = generate_summary(text, num_sentences)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8a776-8397-4b23-b8d8-ad8274457540",
   "metadata": {},
   "source": [
    "Here is the completed code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0facecd5-0bea-435d-9efa-7f414bc715b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the text you want to summarize:  A UN agreement from 1967 says no nation can own the Moon. Instead, the fantastically named Outer Space Treaty says it belongs to everyone, and that any exploration has to be carried out for the benefit of all humankind and in the interests of all nations. While it sounds very peaceful and collaborative - and it is - the driving force behind the Outer Space Treaty wasn’t cooperation, but the politics of the Cold War. As tensions grew between the US and Soviet Union after World War Two, the fear was that space could become a military battleground, so the key part of the treaty was that no nuclear weapons could be sent into space. More than 100 nations signed up.\n",
      "Enter the number of sentences you want in the summary:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "As tensions grew between the US and Soviet Union after World War Two, the fear was that space could become a military battleground, so the key part of the treaty was that no nuclear weapons could be sent into space. Instead, the fantastically named Outer Space Treaty says it belongs to everyone, and that any exploration has to be carried out for the benefit of all humankind and in the interests of all nations. While it sounds very peaceful and collaborative - and it is - the driving force behind the Outer Space Treaty wasn’t cooperation, but the politics of the Cold War. A UN agreement from 1967 says no nation can own the Moon.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "# Function to compute word frequencies\n",
    "def compute_word_frequencies(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    freq_table = defaultdict(int)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and word not in string.punctuation:\n",
    "            freq_table[word] += 1\n",
    "    \n",
    "    return freq_table\n",
    "\n",
    "# Function to score sentences based on word frequencies\n",
    "def score_sentences(sentences, freq_table):\n",
    "    sentence_scores = defaultdict(int)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        for word in words:\n",
    "            if word in freq_table:\n",
    "                sentence_scores[sentence] += freq_table[word]\n",
    "    \n",
    "    return sentence_scores\n",
    "\n",
    "# Function to generate summary\n",
    "def generate_summary(text, n):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_frequencies = compute_word_frequencies(text)\n",
    "    sentence_scores = score_sentences(sentences, word_frequencies)\n",
    "    \n",
    "    # Get the top n sentences with the highest scores\n",
    "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:n]\n",
    "    \n",
    "    return ' '.join(summary_sentences)\n",
    "\n",
    "# User input\n",
    "text = input(\"Enter the text you want to summarize: \")\n",
    "num_sentences = int(input(\"Enter the number of sentences you want in the summary: \"))\n",
    "\n",
    "# Generate summary\n",
    "summary = generate_summary(text, num_sentences)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
